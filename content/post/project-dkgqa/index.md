---
title: "Deep Knowledge Graph Question Answering"
date: 2024-03-29T14:18:02+01:00
author: "Elias Kempf"
authorAvatar: "img/gopher.png"
tags: ["wikidata", "sparql", "kgqa", "question-answering", "llms", "transformer", "nlp", "deep-learning"]
categories: []
image: "img/title.svg"
draft: false
---

Question answering systems automatically provide answers to questions posed in natural language. A more specific type of question answering is knowledge graph question answering (KGQA). Such systems rely on translating a given question into a query over a knowledge graph. These systems often generate many possible queries at once and rank them according to some heuristic. There also exist LLM-based systems that try to directly generate the desired queries. In this project, we want to finetune and evaluate pre-trained LLMs for SPARQL query generation using the Wikidata SimpleQuestions dataset.

## Content

1. [Introduction](#introduction)

2. [Problem definition](#problem-definition)

3. [Approach](#approach)

   - [Setting up a training pipeline](#setting-up-a-training-pipeline)

   - [Initial experiments](#initial-experiments)

   - [Natural language entities](#natural-language-entities)

   - [Scaling to larger models](#scaling-to-larger-models)

4. [Experiments](#experiments)

   - [Adding more LoRA adapters](#adding-more-lora-adapters)

   - [Hyperparameter tuning](#hyperparameter-tuning)

5. [Results](#results)

   - [Finetuned models](#finetuned-models)

   - [Evaluation](#evaluation)

6. [Conclusion and future work](#conclusion-and-future-work)

## Introduction

Large language models (LLMs) have gained much popularity in recent years. They demonstrate impressive capabilities for generating high-quality text and providing quasi-human-like chat-based interactions. Modern LLMs can also be employed for question answering (QA) and will produce reasonable results most of the time. However, we face two obstacles when generating answers to questions with LLMs, i.e., actuality and factuality. For example, answering a question like "Who is the president of the USA?"  requires relatively recent information and an LLM trained five years ago, no matter how powerful, will not be able to give a well-founded answer. Similarly, LLMs tend to provide unfactual answers when queried about information not covered in their training data. Therefore, answers directly generated by an LLM are untrustworthy without prior verification. These limitations make the standalone application of LLMs as question answering systems difficult. However, there is great potential for incorporating LLMs as components of QA systems.

This project presents a method to integrate LLMs into a knowledge graph question answering (KGQA) system. We use an LLM to "translate" questions formulated in natural language into corresponding SPARQL queries over the Wikidata knowledge graph. A knowledge graph consists of triples that each have a subject, a predicate, and an object. Let's again look at our previous example. Here, we want to translate the question "Who is the president of the USA?" to the following SPARQL query (omitting prefixes):
```sparql
SELECT ?target WHERE { wd:Q30 wdt:P6 ?target . }
```
In this query, the identifier wd:Q30 represents the entity "United States of America" in Wikidata. Similarly, wdt:P6 represents the property "head of government". When we evaluate this query on Wikidata, it will return all entities that occur as an object in a triple with the subject "United States of America" and the predicate "head of government". 

Our goal is to obtain answers to our questions by executing our predicted queries using a SPARQL engine that operates on Wikidata, e.g., [QLever](https://qlever.cs.uni-freiburg.de/wikidata). In this project, we work with different pre-trained LLMs that are publicly available and finetune them for generating SPARQL queries from natural language questions on the [Wikidata SimpleQuestions](https://github.com/askplatypus/wikidata-simplequestions) dataset. The following table provides an overview of the number of question-query pairs in the training, validation, and test set of the SimpleQuestions dataset. Note that the dataset was translated to Wikidata from Freebase, and thus not all of the questions have answers in Wikidata.

|  | Training set | Validation set | Test set |
|--|-------------:|---------------:|---------:|
| Total | 34,373 | 4,866 | 9,960 |
| Answerable | 19,481 | 2,820 | 5,621 |

## Problem definition

We will now define the task we want to solve more clearly. We are given a  pre-trained LLM and a list of question-query pairs from the Wikidata SimpleQuestions dataset as input. Questions are formulated in natural language and may contain ambiguities. Queries are formulated in SPARQL and contain Wikidata IDs (we again omit prefixes): 
```python
qq_pairs = [
    (question='Who is the president of the USA?',	query='SELECT ?target WHERE { wd:Q30 wdt:P6 ?target . }'),
    (question='where was neil armstrong born?',		query='SELECT ?target WHERE { wd:Q1615 wdt:P19 ?target . }'),
    ...
]
```
Our task now is to finetune the weights of the given LLM such that, given a simple question, it generates a corresponding SPARQL query. We want the model to generalize to new unseen questions while being as accurate as possible.

## Approach

We will now look at the approach proposed for solving this task. We will discuss the training pipeline, different LLM choices, and some techniques for improving training and model performance.

### Setting up a training pipeline

For this project, we use [PyTorch](https://pytorch.org/), a commonly used deep-learning framework for Python, to finetune our models. Additionally, we also use [Lightning](https://lightning.ai/docs/pytorch/stable/), a framework built on top of PyTorch, which allows building more flexible training pipelines while also avoiding writing a lot of boilerplate code. Lightning offers many improvements, but most importantly, it enables you to scale your training from CPU only to multiple GPUs without any changes to your code.

#### Lightning
On a high level, a Lightning training pipeline consists of three main components: a `LightningModule`, a `LightningDataModule`, and a `Trainer`. The `LightningModule` class encapsulates all logic related to the model we want to train. The `LightningDataModule` class handles all data loading and preprocessing logic. The `Trainer` class manages the whole training loop and moves the model and all data to the correct device. To create your training pipeline, you implement your own subclasses of `LightningModule` and `LightningDataModule` and configure the `Trainer` by passing your desired arguments. A simple training loop then looks like this:
```python
model = MyLightningModule()
dm = MyLightningDataModule()

trainer = Trainer()
trainer.fit(model, datamodule=dm)
```
Consequently, the first step for setting up our training pipeline is to implement a `LightningModule` and a `LightningDataModule` suitable for our task.

#### Working with pre-trained LLMs
As a first step in our Lightning pipeline, we implement a class `TransformerLearner` that inherits from `LightningModule` and will handle everything needed for us to work with pre-trained LLMs from the [ðŸ¤— Transformers](https://huggingface.co/docs/transformers/index) library. We use ðŸ¤— Transformers as it provides easy access to a large set of open-source LLMs and their pre-trained weights for us to experiment with. In particular, the `TransformerLearner` will load the pre-trained models and their tokenizers, set up the optimizer, create a learning rate scheduler, and handle other potential hyperparameters. This encapsulation of the underlying model will make it very convenient to switch and try out different LLMs later (see [here](#scaling-to-larger-models)). Throughout this project, we use the [AdamW](https://arxiv.org/abs/1711.05101) optimizer and a learning rate schedule consisting of a linear warm-up followed by [cosine annealing](https://arxiv.org/abs/1608.03983). Figure <a href="#fig1">1</a> shows an example of such a learning rate schedule. The example uses a maximum learning rate of 1.0, 50 epochs total, and a warm-up for ten epochs.
<figure>
    <center>
    <img id='fig1' src="img/lr-schedule.svg"/>
    <figcaption>Figure 1 - Illustration of the learning rate schedule.</figcaption>
    </center>
    <br>
</figure>

#### Working with the SimpleQuestions dataset
Next, we implement a class `SPARQLDataModule` that inherits from `LightningDataModule` and will handle data loading. The Wikidata SimpleQuestions dataset consists of text files where each line contains four tab-separated values. The first three values are the Wikidata IDs of the subject, predicate, and object of the query. The fourth value is the question we want to predict the query for. The object ID is always the target of our query, i.e., the answer to the corresponding question. Our `SPARQLDataModule` now translates this data to question-query pairs. For example, consider the following three lines:
```
Q2275923	P106	Q40348		What was Seymour Parker Gilbert's profession?
Q522966		P106	Q2526255	What job does jamie hewlett have
Q2568216	R57	Q14949730	What is a film directed by wiebke von carolsfeld?
```
Given this data, our data module produces the following question-query pairs:
```python
qq_pairs = [   
	(question='What was Seymour Parker Gilbert\'s profession?', 	query='SELECT ?target WHERE { wd:Q2275923 wdt:P106 ?target . }'),
	(question='What job does jamie hewlett have', 					query='SELECT ?target WHERE { wd:Q522966 wdt:P106 ?target . }'),
	(question='What is a film directed by wiebke von carolsfeld?',	query='SELECT ?target WHERE { ?target wdt:P57 wd:Q2568216 . }')
]
```
Note that if the predicate ID starts with a P, the target will be in the object position. Predicate IDs beginning with an R encode the inverse of a property in Wikidata. These inverse properties can occur because the SimpleQuestions dataset was initially based on Freebase and only later translated to Wikidata. In such cases, the target and subject ID will switch positions. Afterwards, we replace the R with a P (e.g., the third query in the above example).

As a final step, our data module will use the tokenizer of our pre-trained model to transform our question-query pairs into PyTorch tensors that we can pass to our model. The data module will then return these tensors in a batched format using PyTorch data loaders.

### Initial experiments

Now that we have set up our pipeline for finetuning pre-trained LLMs, we can begin with our first experiments. Before we start, we have to decide on an LLM we want to use. However, we do not need to start with large state-of-the-art models directly. We want to get a good feeling for the task first. Thus, we want a model we can finetune on a single consumer-level GPU. For this reason, we choose the [T5 (Text-To-Text-Transfer Transformer)](https://github.com/google-research/text-to-text-transfer-transformer) model family, which consists of five pre-trained models:

| Model | Parameters |
|-------|-----------:|
| T5-Small | 60 million |
| T5-Base | 220 million |
| T5-Large | 770 million |
| T5-3B | 3 billion |
| T5-11B | 11 billion |

The T5 models use a classic [encoder-decoder transformer](https://arxiv.org/abs/1706.03762) architecture for sequence-to-sequence language modeling. We can generate queries with these models by passing the tokenized questions as input to the transformer encoder and an empty sequence to the transformer decoder. After that, we can use the token distributions predicted by the model to generate our query token by token. Therefore, we also finetune our models for next token prediction. In concrete terms, we use cross-entropy loss to train the model to predict the \\(n+1\\)-th token of the query given the tokenized question and the first \\(n\\) tokens of the query. Let's now try to finetune a T5-Small and T5-Base model. We will use a batch size of 32, a maximum learning rate of \\(10^{-4}\\), one warm-up epoch, and finetune for ten epochs. Figure <a href="#fig2">2</a> shows the training and validation losses for the two training runs.
<figure>
    <center>
    <img id='fig2' src="img/initial-experiments-loss-train.svg" width=800 style="margin: 0"/>
    <img src="img/initial-experiments-loss-val.svg" width=800 style="margin: 0"/>
    <figcaption>Figure 2 - Training and validation loss of the T5-Small and the T5-Base models.</figcaption>
    </center>
    <br>
</figure>
```
```
We can observe multiple things from these plots. First of all, both models are able to learn from the data provided and manage to minimize the training loss. Second, the T5-Base model performs slightly better than T5-Small. Lastly, from the validation loss, we can see that both models start to overfit at some point, but noticeably, the T5-Base overfits more and starts earlier. As overfitting is a common problem when finetuning large models, we do not only keep the checkpoint of the final model but also a checkpoint of the model with the lowest validation loss. The T5-Base model achieved the lowest validation at epoch five in this experiment. Let's see how well it does on the questions from our data module example:

```
What was Seymour Parker Gilbert's profession?
What job does jamie hewlett have
What is a film directed by wiebke von carolsfeld?
```

Note that these questions are from the validation set of the SimpleQuestions dataset, so the model did not train on these. The T5-Base checkpoint from epoch five generates the following output:

```sparql
<pad> SELECT?target WHERE <unk> wd:Q7307816 wdt:P106?target. <unk></s>
<pad> SELECT?target WHERE <unk> wd:Q6123437 wdt:P106?target. <unk></s>
<pad> SELECT?target WHERE <unk>?target wdt:P57 wd:Q314040. <unk></s></s>
```

We can see that the model learned the syntax of our simple SPARQL queries. We now post-process the outputs by removing padding (pad) and end-of-sentence (/s) tokens and replacing unknown (unk) tokens by opening and closing curly braces around the body. We also adjust some spacing for readability:

```sparql
SELECT ?target WHERE { wd:Q7307816 wdt:P106 ?target . }
SELECT ?target WHERE { wd:Q6123437 wdt:P106 ?target . }
SELECT ?target WHERE { ?target wdt:P57 wd:Q314040 . }
```

At first glance, this looks promising. The model has no problems adapting the syntax of our simple queries. It correctly predicts the properties P106 ("occupation") and P57 ("director"). Furthermore, it handles the positions of the target right even with inverse properties (see [here](#working-with-the-simplequestions-dataset)). However, results look much worse when looking at the predicted entity IDs. None of the IDs match the actual entity mentioned in the question:

| Actual entity | Actual ID | Predicted ID | Predicted entity |
|---------------|-----------|--------------|------------------|
| Seymour Parker Gilbert | Q2275923 | Q7307816 | Cerritos, California |
| Jamie Hewlett | Q522966 | Q6123437 | Jai Paul |
| Wiebke Carolsfeld | Q2568216 | Q314040 | Joachim KÃ¼hn |

From the table, we also see that these are not simple one-digit mistakes. There is no noticeable connection between the predicted and the actual IDs. While this result seems discouraging, it is easy to explain. Remember that we took these three examples from our validation set. A quick text search through our training set reveals that none of these three entities nor their IDs exist in the training data. However, the two properties both occur over 100 times. So, the model performs as we would expect under these circumstances. We cannot expect the model to generalize to Wikidata IDs it has never seen during training. We will discuss a solution to this problem in the next section.

### Natural language entities

In the previous section, we saw one problem of working directly with Wikidata IDs. Namely, the model does not generalize to entities not present in the training set. There are more disadvantages. For instance, we need our model to learn as many Wikidata IDs by heart as possible. Yet, if Wikidata adds new entities or properties after we finetuned our model, it will not be able to deal with them. Another motivation for working with pre-trained LLMs is their ability to reason about semantic relationships between words or entities. However, the Wikidata IDs of semantically related entities share no semantic relationship that the LLM will pick up. For example, most pre-trained LLMs will detect a semantic relationship between "Germany" and "Berlin" but not between "Q183" and "Q64". Meaning our current approach leaves a lot of potential unused.

We deal with these disadvantages by replacing Wikidata IDs with natural language entities (NLE). A natural language entity is simply the name of an entity or one of its synonyms. For example, the name of entity Q64 is "Berlin" and has the synonyms "Berlin, Germany" and "Berlin (Germany)". We will also slightly adjust the syntax of queries we want to generate. Consider the again question "Who is the president of the USA?" and the corresponding SPARQL query:
```sparql
SELECT ?x WHERE { wd:Q30 wdt:P6 ?x . }
```
The query will look like this when using natural language entities:
```sparql
SELECT ?x WHERE { United States of America  head of government ?x . }
```
To prevent ambiguities and to make the syntax even more expressive, we will additionally use a tag-based syntax:
```sparql
SELECT <bov>x<eov> WHERE <bob> <boe>United States of America<eoe> <bop>head of government<eop> <bov>x<eov> . <eob>
```
While this arguably looks more complex than before, it is also much more verbose. An LLM will have an easier time relating "president" to "head of state" than to "P6". Natural language entities also generalize better than Wikidata IDs. When asked for the head of government of another country not present in the training data, the model can now make a reasonable attempt by simply using the country name as the entity. By also incorporating synonyms, we can increase the amount of training data. For instance, we can create multiple queries containing different synonyms for a single question. For the above example, we could also have a query with "USA" as the entity instead of "United States of America". We increase our training and validation set size by roughly 20% in this fashion.

We need to implement this approach in a way that we can reconstruct the original SPARQL query with the Wikidata IDs from the ones with NLEs. We can solve this with an inverted index that maps from entity or property names to Wikidata IDs. We can then replace the natural language entities with the corresponding IDs. Converting the tag-based syntax back into proper SPARQL is not too difficult. We discuss two approaches for solving this in a later [section](#sparql-based-evaluation). We will slightly revise our initial problem definition to reflect this adaption: Our task is to finetune the weights of a given LLM such that, given a simple question, it generates a corresponding query containing natural language entities.

Let's now finetune a T5-Base model using natural language entities. We again use a batch size of 32, a maximum learning rate of \\(10^{-4}\\), one warm-up epoch, and finetune for ten epochs. For comparison, we finetune another T5-Base model with the same configuration but using Wikidata IDs. Figure <a href="#fig3">3</a> shows the training and validation losses of both training runs.
<figure>
    <center>
    <img id='fig3' src="img/nle-loss-train.svg" width=800 style="margin: 0"/>
    <img src="img/nle-loss-val.svg" width=800 style="margin: 0"/>
    <figcaption>Figure 3 - Training and validation loss of two T5-Base models trained on Wikidata IDs (WID) and natural language entities (NLE) respectively.</figcaption>
    </center>
    <br>
</figure>

Looking at the plots, we can see that the model trained using natural language entities achieves a much lower loss much quicker. The training and validation losses stay lower throughout the training. These facts suggest that the LLM has an easier time learning to generate our modified queries. We can partially attribute this to the easy-to-predict tokens we introduced with the tag-based syntax. On the other hand, the model can also mostly copy the natural language entities from the question to the query.

### Scaling to larger models

Now that we have established a stable training pipeline and an improved query format, we can start looking into exploiting bigger models. Larger LLMs also come with drastically increased compute and memory requirements. In this section, we will explore techniques to keep training times feasible and discuss good model choices.

#### LoRA and half-precision

[Low-Rank Adaption](https://arxiv.org/abs/2106.09685) (LoRA) is a parameter-efficient finetuning (PEFT) technique. LoRA adds so-called low-rank adapters to the weight matrices of the pre-trained model.
Let's look at an example. Consider a pre-trained weight matrix \\(W \in \mathbb{R}^{d \times d}\\) and an input vector \\(x \in \mathbb{R}^d\\). Normally, we compute the forward pass as \\(x' = Wx\\).
<figure>
    <center>
    <img id='fig4' src="img/nolora.svg" style="margin: 0"/>
    <figcaption>Figure 4 - Classic forward pass of a linear layer (without bias).</figcaption>
    </center>
    <br>
</figure>

When using LoRA, we add two additional weight matrices \\(A \in \mathbb{R}^{d \times r}, B \in \mathbb{R}^{r \times d}\\) where \\(r \ll d\\) is a hyperparameter, the so-called LoRA rank. Then, the forward pass becomes \\(x' = Wx + BAx\\).
<figure>
    <center>
    <img id='fig5' src="img/lora.svg" style="margin: 0"/>
    <figcaption>Figure 5 - Forward pass of a linear layer (without bias) with added low-rank adapters.</figcaption>
    </center>
    <br>
</figure>

The matrix \\(A\\) is initialized randomly, whereas \\(B\\) is initially set to zero. This way, the adapters do not introduce noise to the pre-trained weights. During finetuning, we freeze the weights of \\(W\\) and update only \\(A\\) and \\(B\\). Since \\(r\\) is usually much smaller than \\(d\\), this drastically reduces the number of trainable parameters. After finetuning, we can merge the LoRA weights into our pre-trained model using \\(W' = W + BA\\).

We can apply low-rank adapters to any subset of weight matrices of the model we want to finetune. This way, we can further control the number of trainable parameters of the model. The most common LoRA configuration applies low-rank adapters only to the query and value matrices of a transformer's self-attention modules.

Let's see how LoRA applies to our T5 models. We perform a quick experiment with the T5-Base model. We again use a batch size of 32, a maximum learning rate of \\(10^{-4}\\), one warm-up epoch, and finetune for ten epochs. We compare a full finetune against LoRA with ranks 8, 16, and 32. We add low-rank adapters to query and value matrices only. However, we also finetune the language modeling head of our models, i.e., the output layer of our transformer. Figure <a href="#fig6">6</a> shows the losses of all training runs.
<figure>
    <center>
    <img id='fig6' src="img/lora-loss-train.svg" width=800 style="margin: 0"/>
    <img src="img/lora-loss-val.svg" width=800 style="margin: 0"/>
    <figcaption>Figure 6 - Training and validation loss of a fully finetuned T5-Base model compared to three LoRA variants.</figcaption>
    </center>
    <br>
</figure>

We can see that the full finetune achieves a lower validation loss than the LoRA variants. However, the LoRA variants exhibit no signs of overfitting, whereas the full finetune does. When comparing the LoRA variants against each other, we see that larger LoRA ranks correlate to a lower validation loss. Let's also compare the number of trainable parameters between the four runs:

| Model variant | Total parameters | Trainable parameters | Trainable parameters (adapters only) |
|---------------|-----------------:|---------------------:|-------------------------------------:|
| LoRA r=8 | 248,462,592 | 25,559,040 | 884,736 |
| LoRA r=16 | 249,347,328 | 26,443,776 | 1,769,472 |
| LoRA r=32 | 251,116,800 | 28,213,248 | 3,538,944 |
| Full | 222,903,552 | 222,903,552 | 0 |

Note that the LoRA variants have more total parameters because we added the adapters and a copy of the language modeling head. The language modeling head alone makes up almost 25 million trainable parameters. The added adapters account for less than 1.5% of the total parameters.

In addition to LoRA, we will also explore training our models in half-precision, i.e., using 16-bit instead of 32-bit floats. Using half-precision noticeably reduces VRAM consumption and speeds up training. These advantages of float16 come at the cost of a reduced dynamic range compared to float32. The maximum value representable in float32 is \\((2 - 2^{-23}) \cdot 2^{127} \\approx 3.4 \cdot 10^{38}\\) whereas the maximum value representable in float16 is \\(65,504\\). This reduced precision can theoretically also lead to a decline in model performance. However, in practice, we almost always prefer a larger model with less precision to a smaller model with higher precision. Sometimes float16 computations can under- or overflow during training, causing instabilities or NaN values. This is especially true for older transformer models that were not designed with half-precision in mind. Sadly, this also applies to our T5 model family. We will discuss other, more modern model choices that do not have this problem in the next section.

#### Phi-2 and Mistral-7B

As mentioned in the previous section, our T5 model family does not work well when finetuned in half-precision. Furthermore, the T5 family is also quite old by now. Many more capable models have been released since. We will focus on two recent LLMs: Microsoft's [Phi-2](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) and Mistral AI's [Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/). The Phi-2 model comes with roughly 2.7 billion parameters. Mistral-7B comes with around 7 billion parameters (who would've guessed?). Both models are by no means huge for modern standards, e.g., compare [GPT-3's](https://en.wikipedia.org/wiki/GPT-3) 175 billion parameters. However, they both show good performance compared to models of similar size. Both of these models are pre-trained in half-precision.

In contrast to the T5 models, Phi-2 and Mistral-7B are decoder-only transformers. To generate text with a decoder-only model, we pass the input sequence directly to the decoder and append the predicted tokens to the input. This generation method is also called causal language modeling. We integrate this new functionality using small extensions to our `TransformerLearner` and `SPARQLDataModule` classes. Due to how we designed our learner class, we only need about 30 lines of code each to support the new models. Effectively, we only add the logic for loading the appropriate tokenizer and handling model-specific keyword arguments. We also need to add causal language modeling to our data module. We concatenate our input questions with the desired NLE queries to create our input sequences. During training, the model can see the whole input sequence, including the desired query. Therefore, we need to use causal attention masks to ensure the model does not cheat by looking into the future. These masks prevent the model from attending to tokens to the right of the token it currently tries to predict. 

With these changes, we can start experimenting with the new models.

## Experiments

In this section, we summarize some interesting experiments we did while tuning the performance of our models. In particular, we discuss the effect of adding LoRA adapters to more layers and some basic hyperparameter tuning for Phi-2 and Mistral-7B.

### Adding more LoRA adapters

When playing around with LoRA, we quickly noticed that larger ranks yield better performance. This fact is not surprising, as larger ranks mean more trainable parameters. However, there still was a gap in performance between high-rank LoRA, e.g., \\(r=32\\), and a full finetune (see Figure <a href="#fig6">6</a>).

So, we experimented with adding LoRA adapters to more than just the query and value matrices. We trained three different configurations of the Phi-2 model: a LoRA variant (SA) with adapters added to all weight matrices of the self-attention modules, a LoRA variantÂ (AL) with adapters added to all linear layers of the model (including the MLPs), and a fully finetuned variant. Both LoRA variants used a rank of 32. We used a batch size of 32, a maximum learning rate of \\(10^{-5}\\), five warm-up epochs, and finetune for 50 epochs. Finally, we also used early stopping with ten epochs, i.e., after ten epochs without improvement in validation loss, we stop the training. Figure <a href="#fig7">7</a> shows the loss curves of this experiment.

<figure>
    <center>
    <img id='fig7' src="img/lora-adapters-train.svg" width=800 style="margin: 0"/>
    <img src="img/lora-adapters-val.svg" width=800 style="margin: 0"/>
    <figcaption>Figure 7 - Training and validation loss of a fully finetuned Phi-2 model compared two LoRA variants.</figcaption>
    </center>
    <br>
</figure>

We can see that the fully finetuned model still reaches the overall lowest validation loss. However, adding LoRA adapters to all linear layers noticeably increases performance and gets closer to the performance of a full finetune. Let's again compare the number of trainable parameters between the three runs:

| Model variant | Total parameters | Trainable parameters | Trainable parameters (adapters only) |
|---------------|-----------------:|---------------------:|-------------------------------------:|
| LoRA SA | 2,931,778,560 | 152,094,720 | 21,022,720 |
| LoRA AL | 2,957,992,960 | 178,309,120 | 47,237,120 |
| Full | 2,779,683,840 | 2,779,683,840 | 0 |

Again, the language modeling head makes up most of the trainable parameters, around 131 million. The added adapters account for 0.7% and 1.6% of the total parameters respectively.

The overhead of applying LoRA to all linear layers is small enough, considering the increase in performance. Thus, from now on, we will add adapters to all linear layers when using LoRA.

### Hyperparameter tuning

During our first experiments with the T5-Small and T5-Base models, we usually used a maximum learning rate of \\(10^{-4}\\) which seemed to provide good results. However, when experimenting with Phi-2 and Mistral-7B, we quickly noticed that these models require lower learning rates to achieve good performance. For Phi-2, reducing the learning rate to \\(10^{-5}\\) was enough.

Mistral-7B, however, seemed to profit more from even lower learning rates. So, we deducted another experiment to compare different learning rates. We used LoRA with \\(r=32\\), a batch size of 32, and four learning rates \\(10^{-4}, 10^{-5}, 5 \cdot 10^{-6}\\), and \\(2 \cdot 10^{-6}\\). We have to note that the runs used slightly different numbers of warm-up epochs. The first run used five, the second two, and the last two runs used three warm-up epochs. This could potentially slightly skew our results. However, the change in maximum learning rate has a much greater effect on the learning rate schedule than the number of warm-up epochs. So, our results are still valid. Figure <a href="#fig8">8</a> shows the comparison of the loss curves of the four runs.

<figure>
    <center>
    <img id='fig8' src="img/mistral7b-lr-train.svg" width=800 style="margin: 0"/>
    <img src="img/mistral7b-lr-val.svg" width=800 style="margin: 0"/>
    <figcaption>Figure 8 - Training and validation loss of four Mistral-7B models finetuned with LoRA and different learning rates.</figcaption>
    </center>
    <br>
</figure>

The lower learning rates seem to decrease the model's ability to fit the training data, as seen in the higher training loss. On the other hand, this seemingly also provides some regularization because we see lower validation losses with lower learning rates. We can see a big difference in performance between \\(10^{-4}\\) and \\(10^{-5}\\) whereas between \\(5 \cdot 10^{-6}\\) and \\(2 \cdot 10^{-6}\\) there is almost no visible difference anymore. However, the lowest learning rate still marginally outperforms the next higher learning rate by approximately 0.2%. Considering also the increased regularization effect, we will continue to use \\(2 \cdot 10^{-6}\\) as a learning rate for Mistral-7B from here on out.

Our last experiment also aims at an increased regularization effect. We investigate what effect decreasing the batch size from 32 to 16 has. Except for the batch size, we use the exact same configuration as in the previous experiment with a learning rate of \\(2 \cdot 10^{-6}\\). Figure <a href="#fig9">9</a> shows the results of this experiment.

<figure>
    <center>
    <img id='fig9' src="img/mistral7b-bs-train.svg" width=800 style="margin: 0"/>
    <img src="img/mistral7b-bs-val.svg" width=800 style="margin: 0"/>
    <figcaption>Figure 9 - Training and validation loss two Mistral-7B models finetuned with LoRA and different batch sizes.</figcaption>
    </center>
    <br>
</figure>

We observe no noticeable difference in the training loss. However, we can see in the validation loss that the lower batch size performs better.

Note that we also investigated lowering the learning rate and batch size for Phi-2 but did not observe any comparable performance gains.

## Results

In this section, we discuss the final models we finetuned. We also provide our evaluation results.

### Finetuned models

Throughout this project, we did a lot of experimentation and tuning. We ended up with four model configurations. A LoRA and a full finetune variant for Phi-2 and Mistral-7B. We finetuned the Phi-2 models using a batch size of 32, a maximum learning rate of \\(10^{-5}\\), five warm-up epochs, and 50 epochs in total. For Mistral-7B, we used a batch size of 16, a maximum learning rate of \\(2 \cdot 10^{-6}\\), three warm-up epochs, and 30 epochs in total. Both LoRA variants used a rank of 32 and applied adapters to all linear layers. We finetuned Phi-2 in full-precision, i.e., float32, and Mistral-7B in half-precision. This decision might be surprising at first but the reasoning is quite simple. We can finetune Phi-2 stably using half-precision but did not achieve performance comparable to full-precision. Figure <a href="#fig10">10</a> shows the loss curves of our final models.

<figure>
    <center>
    <img id='fig10' src="img/final-models-train.svg" width=800 style="margin: 0"/>
    <img src="img/final-models-val.svg" width=800 style="margin: 0"/>
    <figcaption>Figure 10 - Training and validation loss of our four final models: a LoRA and a full finetune variant for Phi-2 and Mistral-7B respectively.</figcaption>
    </center>
    <br>
</figure>

We can observe multiple things from the figure. Mistral-7B outperforms Phi-2 noticeably. Also, the full finetune variants achieve a lower validation loss than the LoRA variants. Finally, the full finetune variants start overfitting quickly, whereas the LoRA variants overfit significantly less.

We trained all our models on the [bwUniCluster 2.0](https://wiki.bwhpc.de/e/BwUniCluster2.0). The following table compares the hardware, resources, and time needed to finetune our final models.

| Model variant | Total parameters | Trainable parameters | GPUs | VRAM usage (total) | Duration |
|---------------|-----------------:|---------------------:|-----:|-------------------:|---------:|
| Phi-2 LoRA | 2,957,992,960 | 178,309,120 | 1x A100 | ~51 GiB | 7.9 hours |
| Phi-2 Full | 2,779,683,840 | 2,779,683,840 | 2x A100 | ~134 GiB | 3.6 hours |
| Mistral-7B LoRA | 7,456,690,176 | 214,958,080 | 1x A100 | ~38 GiB | 5.8 hours |
| Mistral-7B Full | 7,241,732,096 | 7,241,732,096 | 1x A100 | ~76 GiB | 4.2 hours |

### Evaluation

We group the evaluation of our models into two parts: a text-based and a SPARQL-based evaluation. The first part of the evaluation consists of text-based metrics that only compare the predicted and expected NLE queries. The second part of the evaluation tries to execute the predicted queries using [QLever](https://qlever.cs.uni-freiburg.de/wikidata) and compares the results to the results of the expected queries. Note that we evaluate all models in half-precision regardless of what precision we trained them in. Also, we always generate queries using beam search with five beams.

#### Text-based evaluation

The first step in our text-based evaluation is a simple query parser that converts a NLE query into a named tuple. For example, consider the query:
```sparql
SELECT <bov>x<eov> WHERE <bob> <boe>Jamie Hewlett<eoe> <bop>occupation<eop> <bov>x<eov>. <eob>
```

The parser produces the named tuple:
```python
(variable='x', triple=(subject='Jamie Hewlett', predicate='occupation', object='x'))
```

On top of this parser, we build three metrics: SyntaxCheck, QueryMatching, and VariablePlacement.

The SyntaxCheck metric verifies that the predicted query follows our pre-defined tag-based syntax. If any syntax violations occur in the prediction, the parser throws an exception, and the metric will count the query as failed.

The QueryMatching metric checks if the predicted query contains the same natural language entities as the expected query. We realize this check using string comparison on the predicate and subject or object (we exclude the variable name). Note that this is quite a strict check and does not consider synonyms. Nevertheless, it was useful during development because it is straightforward to implement and compute. For a better assessment of the question answering capabilities, see the next section on SPARQL-based evaluation.

The VariablePlacement metric checks if the variable is in the correct place in the predicted query. For instance, if the variable is in the object position in the expected query, the check will pass if and only if the variable in the predicted query is also in the object position.

All these metrics yield a binary "pass or fail" result. Thus, for each metric, we compute the percentage of passing queries. We achieve the following results on the Wikidata SimpleQuestions test set:

| Model variant | SyntaxCheck | QueryMatching | VariablePlacement |
|---------------|------------:|--------------:|------------------:|
| Phi-2 LoRA | \\(100.0\\%\\) | \\(68.0\\%\\) | \\(98.6\\%\\) |
| Phi-2 Full | \\(99.9\\%\\) | \\(66.2\\%\\) | \\(98.8\\%\\) |
| Mistral-7B LoRA | \\(\mathbf{100.0\\%}\\) | \\(68.1\\%\\) | \\(99.0\\%\\) |
| Mistral-7B Full | \\(100.0\\%\\) | \\(\mathbf{74.1\\%}\\) | \\(\mathbf{99.1\\%}\\) |

#### SPARQL-based Evaluation

This evaluation is much closer to our actual goal of question answering. However, as it requires executing queries over Wikidata, it is also more expensive to compute. As a first step, we pre-compute the results of the ground truth SPARQL queries from the Wikidata SimpleQuestions test set using QLever. We then convert the predicted NLE queries into SPARQL and execute them against QLever. This yields a set of expected and a set of retrieved entities for each query. We then compute a per-query [F1 score](https://en.wikipedia.org/wiki/F-score) from these two sets. Finally, we average these scores over all queries. We also compute the accuracy, i.e., the fraction of questions for which the set of expected and the set of retrieved entities are equal. We compare two ways of converting the NLE queries into SPARQL.

First, we use a basic conversion approach based on an inverted index. We refer to this approach as inverted index conversion (IIC). We build an inverted index that maps from natural language entities to Wikidata IDs. This approach does not use any fuzzing to deal with typos or other similar mistakes. Therefore, it can happen that we cannot map a natural language entity back to a Wikidata ID. In such cases, we return a score of \\(0\\). We also return a score of \\(0\\) if the NLE query has an invalid syntax.

The second way uses so-called constrained prefix decoding (CPD), developed and implemented by Walter. With this approach, the decoding process is restricted to only predicting valid natural language entities. These are exactly those entities that are contained in our inverted index. This is implemented using a prefix index. This way, we can ensure that the queries we decode always translate to a valid SPARQL query. Furthermore, a feature called subgraph constraining is used. Subgraph constraining prevents predicting combinations of natural language entities that do not occur in Wikidata. For example, if we predict Albert Einstein as a query subject, subgraph constraining would only allow predicting predicates that occur with Albert Einstein in Wikidata.

We achieve the following results on the Wikidata SimpleQuestions test set:

| Model variant | F1 score (IIC) | F1 score (CPD) | Accuracy (IIC) | Accuracy (CPD) |
|---------------|---------:|---------:|---------------:|---------------:|
| Phi-2 LoRA | \\(76.7\\%\\) | \\(77.2\\%\\) | \\(76.4\\%\\) | \\(76.3\\%\\) |
| Phi-2 Full | \\(78.4\\%\\) | \\(79.1\\%\\) | \\(78.1\\%\\) | \\(78.2\\%\\) |
| Mistral-7B LoRA | \\(79.9\\%\\) | \\(87.8\\%\\) | \\(79.7\\%\\) | \\(86.7\\%\\) |
| Mistral-7B Full | \\(\mathbf{81.2\\%}\\) | \\(\mathbf{88.2\\%}\\) | \\(\mathbf{81.0\\%}\\) | \\(\mathbf{87.1\\%}\\) |

We see that the full-finetune variants outperform their respective LoRA counterparts and that Mistral-7B performs better than Phi-2. We also notice that using constrained prefix decoding yields significant improvements for the Mistral-7B models andÂ almost none for Phi-2. A brief investigation showed that even with constrained prefix decoding, the Phi-2 models sometimes fail to predict a valid entity. This suggests that in those cases where the Phi-2 models fail, they will also fail to choose the correct entity when given a restricted set of tokens.

We also want to briefly compare the results of our best model, Mistral-7B Full, against a previous work at this chair. David Otte worked on question answering on Wikidata using, among others, the Wikidata SimpleQuestions dataset (see his [blog post](https://ad-blog.informatik.uni-freiburg.de/post/question-answering-on-wikidata/) or his [thesis](https://ad-publications.cs.uni-freiburg.de/theses/Bachelor_David_Otte_2023.pdf)). His approach was based on matching the entities in the question, generating candidate queries, and finally filtering and ranking the candidates. We compare the average F1 score, the accuracy, and the average duration for query generation of our Mistral-7B Full model with and without constrained prefix decoding to his results on the Wikidata SimpleQuestions test set. Since his work, 13 questions from the test set became not answerable due to changes to the Wikidata knowledge graph. Thus, he evaluated his approach on 5,622 questions, whereas we only evaluated 5,609 questions.

| Approach | F1 score | Accuracy | Average duration per query |
|----------|---------:|---------:|---------------------------:|
| David Otte (Project) | \\(80.0\\%\\) | \\(79.0\\%\\) | 1.60 seconds |
| David Otte (Thesis) | - | \\(81.6\\%\\) | 0.49 seconds |
| Mistral-7B Full (IIC) | \\(81.2\\%\\) | \\(81.0\\%\\)  | 1.21 / 0.17 seconds |
| Mistral-7B Full (CPD) | \\(88.2\\%\\) | \\(87.1\\%\\) | 3.21 / 2.61 seconds |

For the speed comparison, we measuredÂ theÂ average generation duration per queryÂ of our modelÂ on an NVIDIA RTX 4090 GPU. Since we do not know what batch size Otte used for evaluation, we report the duration for our approach using batch sizes 1 and 16 (first and second duration respectively). In his thesis, Otte did not report an average F1 score,Â butÂ we see that the accuracy metric yields similar results and still allows for a comparison. We can see that our full deep-learning approach can achieve comparable accuracy while using inverted index conversion. When using constrained prefix decoding, we can noticeably improve accuracy over Otte's results. In terms of speed, inverted index conversion is faster than Otte's approach when using batched inference but slower when processing questions one by one. Constrained prefix decoding is significantly slower in both cases. Note that we only report the average generation time per query. If we also want to execute the predicted query against QLever, the duration increases by approximately 0.05-0.1 seconds for both conversion methods. Lastly, we want to mention another advantage of our approach. Since we use a fully deep learning-based approach, it is comparatively easy to extend our approach to more complex queries.

## Conclusion and future work

With this project, we demonstrate the potential of LLMs for query generation in knowledge graph question answering systems. We build a flexible and extensible training pipeline for finetuning LLMs on the Wikidata SimpleQuestions dataset. We apply two recent models on this task, Phi-2 and Mistral-7B. We achieve promising evaluation results even with basic conversion techniques. Finally, we show that using constrained prefix decoding improves results even further. Overall, our fully finetuned Mistral-7B yields the best results.

Still, our approach has some limitations. Our models can only deal with simple questions, i.e., they only generate queries consisting of a single triple. Furthermore, we only consider the Wikidata knowledge graph. However, these limitations are not inherent to our approach. We use a full deep-learning setup that can be used to generate arbitrary text. Therefore, it is straightforward to extend our approach to different datasets and other types of knowledge graphs. In particular, applying our approach to more complex questions and queries is a promising direction for future work.
